# -*- coding: utf-8 -*-
"""Submission TimeSeries(Power_Plant).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15dbsm4Tg6yvgZOlWgE2G7gwH1RW2cTZa
"""

from google.colab import drive
import os

drive.mount('/content/gdrive/')
os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/My Drive/Kaggle'
zipfile = 'hourly-energy-consumption.zip'
extfile = 'time_seriesSubmissioPowerPlant'

cd /content/gdrive/My Drive/Kaggle

!kaggle datasets download -d robikscube/hourly-energy-consumption

import zipfile as zf

try:
  extFile = zf.ZipFile(zipfile)
  extFile.extractall(extfile)
  extFile.close()
  print('extraksi dataset selesai')
except(FileExistsError,FileNotFoundError):
  print('Dataset tidak ditemukan')
except:
  print('terjadi kesalahan')
finally:
  print('operation terminated')

os.listdir(extfile)

dir = os.listdir(extfile)
datasetdir = os.path.join(extfile,'PJME_hourly.csv')

print('Power Plan directory: {}'.format(datasetdir))

import pandas as pd

loadData_one = pd.read_csv(datasetdir,parse_dates=['Datetime'])
loadData_one

loadData_one.isnull().sum()

powerplan = loadData_one.rename(columns={'Datetime':'datetime','PJME_MW':'pjm_mw'})

normalizedata = powerplan
powerplantqty = len(normalizedata)
print('banyak dataset: {}'.format(powerplantqty))
normalizedata.head(10)

dataMae = (max(normalizedata.pjm_mw)-min(normalizedata.pjm_mw))
split1 = 0.10
split2 = 0.032
dataMaesix = int(dataMae * split1)
dataMaetfv = int(dataMae*split2)
mae10 = dataMaesix / 10000
mae06 = dataMaetfv / 10000

print('Skala maximal data: {}'.format(dataMae))
print('skala data {}%: {}'.format((split1*100),dataMaesix))
print('skala data {}%: {}'.format((split2*100),dataMaetfv))
print('skala data {}% mae: {}'.format((split1*100),mae10))
print('skala data {}% mae: {}'.format((split2*100),mae06))

import matplotlib.pyplot as plt

datetime = normalizedata.datetime.values
output = normalizedata.pjm_mw.values
Title = 'Power Plan Output'

def plotting(Xaxes=[],Yaxes=[],title=''):
  plt.figure(figsize=(20,5))
  plt.plot(Xaxes,Yaxes)
  plt.xticks(rotation=45)
  plt.title(title,fontsize=20)
  plt.xlabel('date',fontsize=12)
  plt.ylabel('Power Plan output',fontsize=12)
  plt.show()  

plotting(datetime,output,Title)

normalizedata.info()

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import datetime

data = normalizedata.values

timePowerPlant = []
PowerOutput = []

for i in range(len(data)):
  timePowerPlant.append(data[i,0])
  PowerOutput.append(data[i,1])

timePowerPlant = pd.DataFrame({'datetime':timePowerPlant})
PowerOutput = pd.DataFrame({'pjm_mw':PowerOutput})

def normalize(normalData):
    scaler = MinMaxScaler(feature_range=(-1, 1))
    dates_scaled = scaler.fit_transform(normalData.values)
    return dates_scaled


time = normalize(timePowerPlant)
pwr = normalize(PowerOutput)  
timeFinal = []
pwrFinal = []
for i in range(len(time)):
  timeFinal.append(time[i,0])
  pwrFinal.append(pwr[i,0])

finalDataPowerPlant = pd.DataFrame({'datetime':timeFinal ,'pjm_kw':pwrFinal})

finalDataPowerPlant

from sklearn.model_selection import TimeSeriesSplit
import numpy as np

tscv = TimeSeriesSplit(n_splits=4)#20% validation
data = finalDataPowerPlant.values

for tr_index,val_index in tscv.split(finalDataPowerPlant):
  traindata,testdata = pd.DataFrame({'datetime':data[tr_index,0],'pjm_mw':data[tr_index,1]}) , pd.DataFrame({'datetime':data[val_index,0],'pjm_mw':data[val_index,1]})  
n_steps = 1
n_feature = 1

data_train = traindata.pjm_mw.values
data_val = testdata.pjm_mw.values

lenData = len(finalDataPowerPlant)
print('total dataset awal: {}'.format(lenData))
print('train data qty: {}'.format(len(traindata)))
print('test data qty : {}'.format(len(testdata)))

traindata.shape

datetimeT = traindata.datetime.values
powerT = traindata.pjm_mw.values
datetimeTs = testdata.datetime.values
powerTs = testdata.pjm_mw.values

plt.figure(figsize=(20,5))
plt.plot(datetimeT,powerT,label='training')
plt.plot(datetimeTs,powerTs,linestyle='dashed',label='validation')
plt.legend()
plt.show()

import tensorflow as tf
from tensorflow import keras


def windowed_dataset(series,windows_size,batch_size,shuffle_buffer):
  series = tf.expand_dims(series,axis=-1)
  ds = tf.data.Dataset.from_tensor_slices(series)
  ds = ds.window(windows_size + 1, shift=1, drop_remainder=True)
  ds = ds.flat_map(lambda w: w.batch(windows_size + 1))
  ds = ds.shuffle(shuffle_buffer)
  ds = ds.map(lambda w: (w[:1], w[1:]))
  return ds.batch(batch_size).prefetch(1)

train_pwr_set = windowed_dataset(data_train,windows_size=6,batch_size=100,shuffle_buffer=1000)
test_pwr_set = windowed_dataset(data_val,windows_size=6,batch_size=100,shuffle_buffer=1000)

train_pwr_set.element_spec

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Flatten
from tensorflow.keras.layers import Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import Huber

model = Sequential()
model.add(Bidirectional(LSTM(1024,activation='relu',return_sequences=True),input_shape=(n_steps,n_feature)))
model.add(Bidirectional(LSTM(1024,activation='relu', return_sequences=True)))
model.add(Bidirectional(LSTM(512,activation='relu', return_sequences=True)))
model.add(Bidirectional(LSTM(512,activation='relu', return_sequences=True)))
model.add(Bidirectional(LSTM(60,activation='relu')))
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(2, activation='relu'))
model.add(Dense(1))

optimizer = Adam(lr=1.000e-04)

model.compile(
    loss = Huber(),
    optimizer = optimizer,
    metrics=['mae']
)

model.summary()

import tensorflow as tf
MAE_TRESHOLD = mae06

class Callback(tf.keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs={}):
    if(logs.get('mae') < MAE_TRESHOLD):
      print('\n  MAE < {} or {}% Scala Dataset.\n End the Training.'.format(MAE_TRESHOLD,(split2*100)))
      self.model.stop_training = True
     

callbackval = Callback()
saveLog =  tf.keras.callbacks.CSVLogger('trainingSubmissionTimeSeries(Power_plant)Log.csv',separator=',')

with tf.device('/device:GPU:0'):
  history = model.fit(
                    train_pwr_set,
                    epochs=15,
                    steps_per_epoch=100,
                    validation_steps=20,
                    batch_size=100,
                    validation_data = test_pwr_set,
                    callbacks = [callbackval,saveLog],
                    verbose=1
                    )

model.save('TimeSeriesmodel(PowerPlant).h5')
print('Training selesai')

mae = history.history['mae']
loss = history.history['loss']
epoch = range(len(mae))
val_mae = history.history['val_mae']
val_loss = history.history['val_loss']


def Trainplotting(Xaxes=[],Yaxes=[],Yval =[],title=''):
  plt.figure(figsize=(10,6))
  plt.plot(Xaxes,Yaxes,label='mae')
  plt.plot(Xaxes, Yval,label='Loss')
  plt.legend()
  plt.title(title,fontsize=20)
  plt.show()
 
Trainplotting(epoch,mae,loss,title='Training Accuracy and Loss')
print('\n\n')
Trainplotting(epoch,val_mae,val_loss,title='Validation Training and Loss')

mae_bool = True if mae[len(epoch)-1] < mae10 else False
print('nilai data tertinggi: {}'.format(dataMae))
print('Actual {}% skala data: {}'.format((split1*100),dataMaesix))
print('Target {}% skala data: {}'.format((split2*100),dataMaetfv))
print('MAE {}% = {}'.format((split2*100),mae06))
print('MAE {}% = {}'.format((split1*100),mae10))
print('MAE < {}% = {}'.format((split1*100),mae_bool))